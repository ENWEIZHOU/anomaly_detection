Abstract:
This paper demonstrates and describes some methods related to concepts and techniques of cybersecurity. This paper explores and analyzes anomalies in a real data set. The data set we used to detect the anomalies is from some parts of the U.S electrical power grid. It monitors household power consumption with few features and collected time. Method in the paper advocate solutions based on constructing methods and models to distinguish normal data and anomalous data. We find different features of the data and we also make plots to indicate the result precisely of each method or model and ferret the correlation among features and infer reasons causing these anomalies.

Introduction
The increasing number of cyberattacks causes considerable damage for finance and physics that influences our lives and business badly. Physical harm, which is worst case, such as the critical infrastructures, which refers to water management systems, electric power grids, etc. The electrical power grid includes variety of power consumption and the household power consumption is a part of it.

Nguyen and Nahrstedt suggests accidental damages on the electric power grid and took a lot of time to be repaired, which shows that it is a very serious problem if those were attacks instead of the accidents. It is hard to distinguish if the system is malfunctioning or there has been a virus [7]. Malfunctioning and virus even threaten human safety ultimately.. 

Analysis is to find abnormal data called anomaly. By given the household power consumption data file from the electrical power grid, it is crucial that to seek the abnormal power consumption at a specific time or in a duration defined as anomaly. We Analyze reasons causing these anomalies over different contexts such as peak period and accident and also spread warnings about until being fixed.

Background
Anomaly detection is a technique used for finding patterns in data deviated from normal behavior. The common term used for these data points that do not conform to normal behavior is called anomalies or outliers. Different anomaly detection techniques have been developed based on specific application domains. Anomalies in data usually translate to significant and critical actionable information in different domains. Anomaly detection is used in a variety of domains such as finance or insurance where it is used for fraud detection, health where it is used for detecting health problem, intrusion detection for cybersecurity in companies and many other domains to detect anomalous behavior. Each domain has different data values and applying the wrong technique will lead to incorrect findings since there are contextual anomaly. Contextual anomalies are data instance that are anomalous in the specific context.

Three categories of anomaly detection technique we implemented: Supervised anomaly detection, Semi-supervised anomaly detection and Unsupervised anomaly detection. Supervised anomaly detection requires a labeled data set that labels normal and abnormal data. The labeled data set will train a classifier to detect anomalies in a different data set. It is difficult to get a label data set since it is costly and there is a much lower chance of anomalies in a data set. In semi-supervised anomaly detection, we build a model based on the normal behavior from the training data set that has been labeled. Once the model is built, test data is fed into the model and the likelihood will determine of the data is normal or anomalous. Unsupervised anomaly detection is similar to semi-supervised version but the model was based on assuming that majority of the train data will be normal.

A couple of things such as Noise, Novelties and Outliers that has to be looked out for in a large data set. Because value may cause hindrance while analyzing the data. Noise is data that adds no value to data analysis such as random data that do not conform. Novelty are patterns in data that look anomalous at first but after further analysis they are incorporated into the normal model. Outliers is a point in the data set that is distant from the rest of the data.

The Problem Description
The problem for this project is detecting unknown attacks using Anomaly-based IDS such as point anomalies, contextual anomalies in individual household electric power consumption based on the data set being provided, and being able to find patterns on data instances which don’t conform with the normal behaviour. We will also be looking for early detection and warning about suspicious and potentially harmful anomalies such as the anomalies which mitigate the impact of attacks. Other problems we will be looking at when we are analyzing our data set include missing data, labels to indicate if an specific instance is an “anomaly” or “normal”. 

Electric power grid is very vulnerable to the cyber attacks. By working on the problems described, we can reduce and produce warning about suspicious attacks to avoid physical damages which can affect human resources.

The Methodology used for solving the problem
1.	Finding the Problem Scope
2.	Finding the features in the data set.
3.	Splitting the Data set( Into Windows)
4.	Exploring the Data set (Min, Max, Standard Deviation, Mean, Median,Correlation)
5.	Anomaly Detection :
	5.1	Moving Average Function to detect Point Anomalies.
	5.2 	Finding outliers in the Data set to detect Point Anomaly Detection)
	5.3 	Hidden Markov Model to detect Contextual Anomalies.

1.Understanding the Problem Scope
Before delving into the large data set, we needed to understand the problem at hand and seeing how our solution will help the problem. The electric power grid throughout North America  is connected by regions as seen in Figure 1. It shows that the electric power grid is vulnerable because the communication between the industries are standardized across different countries which limits security [4]. Electric power grid is a critical infrastructure since power is delivered to a large region. If a perpetrator can cut off any connections, this can lead to blackouts across regions. The infrastructure in the power grid is using Supervisory Control and Data Acquisition (SCADA) systems which is the underlying architecture that is powering everything. If anything happens to the SCADA system, it can cause catastrophic issues affecting the power grid. Only the software which leads to these attacks. We are focusing on finding a solution to detect and warn us about suspicious activity and anomalies to have the upper hand during a cyber attack by coming up with countermeasure. By creating a model of existing data, we can detect potential anomalous behavior. We can also feed new data into our model to determine if there are suspicious and anomalous activities. 

2. Understanding the Features
The project start from first researching and understanding the features in the Data set that we were not familiar with.  After conducting a few days of research, we defined the following features:

Global active power: is the real power consumption by appliances not included in the sub meters. It is household global minute-averaged active power and its unit is kilowatt.[8] 

Global reactive power :is the  household global minute-averaged reactive power with the unit of kilowatt. It refers that the power moves forward or backward that is not used or leaked. Compared to active power, it is the imaginary power consumption and the active power is the real power consumption. [8]

Global intensity: is the power consumption in a amount level or strength. It is the household global minute-averaged current intensity and its unit is ampere. Sub metering also represents the power consumption and each sub metering defines the power consumption by a specific type of appliance with unit watt-hour of active energy. [8]

Voltage: is the minute average voltage that is used in the household. The higher the voltage means there is a high flow of electrical current.[8]

Sub Metering 1: refers that the power are consumed by appliances in the kitchen, such as microwave, oven dishwasher. [8]

Sub Metering 2: refers the power consumption of the type of appliances related to the laundry room, and for instance the power is consumed by washing-machine, tumble-drier. [8]

Sub metering 3: are the electric heating system as air-conditioner and water heater.[8]

3. Splitting the Data
We created functions to extract (Months, Days, Years, Minute and Hours) which helped us to split our date and time into new columns .This was the most challenging part,as was we needed to understand the different data types (Classes of the fields in R) and how to convert that to a datatype which can be used for extracting records from the data set. After adding extra columns which were defined by the new functions that we made, it made it easy to split the data. As a group we decided to split the data into seasons ,and two different time windows (Summer, Winter and Spring) which we defined as;
●	Summer (May-August )
		Summer Evening (6:00PM- 11:59PM)
		Summer Day (6:00AM- 5:59PM)
●	Winter (September-December)
		Winter Day (8:00AM- 5:59PM)
		Winter Evening (6:00PM-11:59PM)
●	Spring(January-April)
		Spring Day (8:00 AM - 5:59PM)
		Spring Evening (6:00PM-11:59PM)
The authors of this project also explored different windows in the season such as Monday-Friday and Saturday-Sunday in the Summer, as the patterns of Monday- Friday were similar, and the pattern for saturday was slightly different to sunday.
4. Exploring the Data set
We read an article about power demand throughout a day in the New England Region on U.S. Energy Information Administration website [1], we determined to split the data based on the chart seen in Figure 2. From Figure 2, we can see that the peak demand was between 6:30 PM-7:00 PM so we made sure to include those datapoint when exploring the data. Since the peak demand was during the 6:30 PM time period, we decided to extract the after work hours to do further analysis. We choose after work hour as 6:00PM-11:59PM assuming that this period would lead to the highest power consumption with everyone being home after work.

After determining the time frame we would use, we dug deeper by analyzing all the data between 6:00PM-11:59PM. We plotted the charts of Global Active Power (Figure 3), Global Reactive Power (Figure 4), Global Intensity (Figure 5),and Global Voltage (Figure 6) and split it into months and years to look for patterns. Each boxplot in the month consist of ~30 data points (# of days in the month), each data point is the average consumption between 6:00PM-11:59PM. We extracted Mean, Median, Standard Deviation, Min ,and Max from each featured to see if there are any decisive manner to choose one feature for further examination (Figure 7). We also extracted the same information from the test data set to see if there are any major differences compared to the training data set (Figure 8). 

From the information gathered from Figure 7, it was obvious that Voltage was not a good feature to choose since the range was much smaller than the other 3 features. It was difficult to determine which of the remaining 3 features would be the best to use for detecting anomaly. In the end we decided to use Global Active Power since it is the true power being consumed [2]. We also saw some increases in Mean, Median and Min and decreases in standard deviation and Max when comparing the training data set to the test data set (Figure 8). We did not choose Global Reactive Power since it is an imaginary power consumption which would not provide us with accurate results. We did not choose Global Intensity since the values are dependent on the submeters which would require more variables involved in data analysis. We also compared the pairs of features to look for correlations that could be used for multivariate HMM (Figure 9). We noticed that Global Active power and Global Intensity had the highest correlation of 0.7059081.

After narrowing down the features to Global Active Power and 6:00PM-11:59PM range, we still had a large amount of data to deal with. By looking at Figure 3, we see that there is a dip in Global Active Power during the summer months of May to August. We decided that we should do some further analysis in the summer months since the values are much lower during this time frame. We thought that it would be easier to detect anomalies if the values are lower than other seasons and months. We extracted the summer months and got the data from 2007-2009 as seen in Figure 10. We noticed some odd behaviors in some of the months so we used this subsetted data for our anomaly detection methods.

5. Anomaly Detection
5.1 Moving Average:
The moving average method is that, it chooses a window with a duration as size and keep sliding the window by a specific step size until the window reach the last data point in the file. The method calculates average data in each window ,and use it to predict if each next data outside the window is normal or anomalous by comparison of the difference and a threshold. The window size includes a number of data and it determines stability of each average in each window. We implement the moving average method firstly by determining a window size as 15 minutes and step size as 1 minute to travel the data. We also considered a circumstance that may cause error prediction, and that is the next data is the first data in the next day. It is not reasonable to use the last window in the former day to predict a data in a new day. Therefore we make each new start of prediction from the first 15 minutes of each day. The threshold value affects the quantity of normal data and anomalies significantly in this method. If the threshold is too small, the method will sensitively identify the normal or anomalous data. If the threshold is too large, the method is not able to rightly approach the correct detection, in other words, the method will determine a large number of normal data and small number of anomalies than exact. To decide the threshold value reasonably and convincingly, we use the statistical method to approach, which is “68-95-99.7” rule by setting the threshold dynamically based on each window. We set each threshold as three times of the standard deviation in the current window.

5.2 Finding Outliers 
To detect outlier in our data set, we determined the min and max value in our train set and applied that knowledge to our test set. If the values in the test data set are smaller than the min or bigger than the max then we classify these value as point anomalies. If we detect values that are not valid such as “NA” then we classify these as noise and the remaining data are considered normal. Since the the detection based on corresponding time between training dataset and test dataset will be more accurately, we find the min and max of a specific time and detect the point of the corresponding time in the test dataset.
5.3 HMM Model
The implementation of the HMM model was quite challenging, and time consuming. We wanted to have about 5 different models, and we achieved this by selecting one weekend day in the winter and summer, and one weekday in the summer. We focused on three different time periods which were 6:00PM-11:59PM for summer and winter, and in the morning from 8:00AM-5:59PM for winter, and 6:00am-5:59pm in the Summer. The reasons on why we picked this time window was based on the pattern trends from the data exploration section of our study.

1. We first did a cross validation of our Train Data set and split it into two sets ( Train and 
validation). We split the data with 70% Test data and 30% for Validation data. Furthermore, we took extra precaution when splitting this data to make sure we didn’t have one day of data being split between the two sets, as this is a time series data.

2. We created a Test Model Function which was left overnight to test ranging from 2 states to 17. It recorded the Log Likelihood, BIC of the Train set and Log Likelihood of the validation set. The summary gave us the BIC of each of the states for the Train Set.

3. We put the numbers into an excel spreadsheet to start comparing our values and finding the best state, as seen in Figure 12 below.

4.To Normalize the Log Likelihood results we divided the result for the train and validation by the number of observations that were included in each result. The rationale behind normalizing the log likelihood is because the pattern length of each observation is different, and getting the average gives you a more accurate result when deciding the train and validation and also the train and test.

5. The way we achieved in finding the best state was by finding a high likelihood and low BIC.We compared each state Likelihood and BIC and made sure we avoided selecting a state which was overfitting our data.

6. After conducting this test we compared the Normalized log likelihood for the train and validation set .We used Microsoft Excel to record the difference between the two Log Likelihood (Train and Validation) to better assist in choosing a state, as seen in Figure 11 below.

7. After finding the best model with the best number of states we then used the “Train” data set been provided and compared the Normalized Log Likelihood to see if the train set had anomalies 

Results for Anomaly Detection
In the sections below we will describe the solutions we got after using the Moving Average Algorithm, Outlier Detection and HMM model in R.
Point Anomalies Result 

Min-Max (Outlier Detection)
The Feature we picked for this was global active power and our rationale behind this is because it represents the true actual power. We also found during the data exploration that the test dataset had a lower min compared to the train dataset hence, we have more anomalies that were less than the min therefore, the authors believed it would be a good idea to explore this.

Moving Average
The results below are what we got for using the moving average technique. We used the train set to help us find the anomalies as it had more data, and it also helped us analyze if our function was working correctly.  The feature we used for the Moving Average was the global active power, as we found this the most useful feature of the dataset. We assumed it was fine to use the train data for this section only. From Figure 14, which illustrates Anomalies between Monday-Friday in the Summer, we can see that between 7:00AM-7:59PM there were over 200 anomalies, and between 8:00 PM-8:59 PM and 10:00PM-10:59 PM with about 150 anomalies. In Figure 15, we see the normal data is consistent between Monday-Friday. The Monday-Friday Dataset consisted of 125280 observations and the Saturday and Sunday consisted of 5180 observations.

Contextual Anomalies using the HMM Model
In the models below we used the global active power feature, as mentioned above in the data exploration section of the report, as we found this to be the most useful feature in the data set because it represents the real power consumption. We decided to work with univariate HMM, as during the data exploration we noticed that this specific feature had patterns in the data, and we wanted to analyze this feature by finding more patterns that don’t conform with the normal behaviour. In this part we simplified our models by picking one day in the weekend, and one day in the weekday to find contextual anomalies. We did Summer and Winter Night for Saturday and Summer Evenings for Wednesday. We did an analysis for each model as seen in Figure 11 for the models before deciding the number of states it requires.

Model 1: Findings for Summer Evening Data set(May- August) 6:00PM-11:59PM for all Saturday’s (Feature: Global Active Power)
The Log Likelihood of our Train Data set was -1763.342 and the BIC was 4699.596 with 10 states. After normalizing the Log Likelihood by the number of observations we got a result of -33.27060377 for the train data set. We then used the parameters of our train data set to compare it with the test data set been provided ,and the results we got for the Log Likelihood was -10788.54 and after normalizing this result we got a Log Likelihood of -599.3633333. There was a difference between the two log likelihoods, and this was a clear indication that there were contextual anomalies in the test data set. 

Model 2: Findings for Summer Day Data set( May-August) 6:00AM-5:59PM for all Saturdays
The Log Likelihood of our Train Data set was -1347.924, and the BIC was 4357.878 with 12 states.  After Normalizing the result of our Train Log Likelihood I got a result of -38.51211429. We checked the Log Likelihood of our Test Data set and got a log likelihood of -16231.66, and after normalizing the result we got the log likelihood to be -901.7588889. The difference in the log likelihood  shows that there were contextual anomalies in our Test Data set between this time window.

Model 3: Finding Contextual anomalies for Summer (May-August) 6pm-11:59pm for all Wednesday’s
We decided to build a model for Wednesday’s to detect anomalies in the evening hours after work. The reason why we picked wednesday is because we felt that it’s the midweek, and and we wanted to explore the patterns between this time window. We can see a difference between the Log Likelihoods of the Train and Test, therefore this indicates there were contextual anomalies in this specific window. Our HMM Train model had a log likelihood of -1391.432 and a BIC of 4976.593.

Model 4: Finding for Winter Day (September- December) 8:00AM-5:59PM for all Saturday’s
The Log Likelihood of the train data set was -4073.709 and BIC was 9252.607 with 10 states.  After normalizing the log likelihood we got -226.317167. We compared our Log Likelihood to the train data set been provided, and we got a Log Likelihood of -15405.97 and after normalizing this log likelihood we got -962.873125 therefore, indicating that our test data set for the given window had contextual anomalies.

Model 5: Finding for Winter Night (6:00PM-11:59PM) for all Saturday’s
After training the model we picked the best model with 12 states as it was the one with the best Log Likelihood of -8152.175 and lowest BIC, and after Normalizing the Likelihood we got  -159.8465686.  For the Test Data set we got a Log likelihood of -8859.285 and after normalizing it we got -553.7053125. There was a small difference between the two values which indicates that they were few contextual anomalies in our test data set. This model had a few flaws, as there was some Missing Data in the dataset which may have affected the result accuracy. 

Major problems encountered in the course of the project
We encountered several problems through the project. 
1.	Understanding the R language and the syntax. We had never worked with data frames before so understanding this concept was quite challenging. 
2.	Understanding the features of the Data set took us some time, as we needed to know which feature was most beneficial to us, and this required data exploration to check for average, standard deviation, correlation etc.
3.	Splitting the data into seasons took us time. It required us to create extra columns in our data set to be able to extract certain data records from the frame. E.g splitting the time into hour, minute and seconds.
4.	Converting date format from factor class to numerical
5.	Understanding the Moving Average concept, and how to use it to find point anomalies.
6.	We first had issues deciding to pick a certain window frame. Eg. Seasons, Weekends, Fridays nights
7.	Selecting the number of states for our HMM took use time, as we had to look for a high log likelihood, and low BIC which took us some time.
8.	Splitting the test data into (test and validation set) for the HMM model.
9.	How to train the model using the functions in R.
10.	Positive Log Likelihood in HMM models, not enough research to show if positive log likelihoods doesn’t lead to overfitting.

Conclusions
 By using the moving average, min and max to detect outliers, and HMM Models, we were able to make the anomaly detection process easier.We managed to find anomalies in our test dataset using those methods.  There were additional steps that were required to solve this problem. It could not have been done without understanding the problem scope, understanding the features in the data set, splitting the data and exploring the data set. The methodology we used was integral towards the success of our final results listed above.

We achieved in finding the point anomalies through two techniques min-max and moving average. We detected the outliers in the Summer between Monday-Friday and Saturday-Sunday , and as seen in Figure 12 and 13. The Figures illustrates anomalies over a period of 4 months, and each bar represents anomalies in a particular hour. From the graph above we observed more anomalies between the following time windows. 7:00AM-7:59AM , 8:00PM-8:59 PM , and 10:00PM-10:59 PM.

The contextual anomalies were found by building HMM models, as seen above in the methodology section.  We managed to train five different models. We found contextual anomalies in our summer evening data between (6:00PM-11:59PM), our log likelihood of the train dataset was -33.27060377 after normalizing, and the test dataset had a log likelihood of -599.3633333 after normalizing. The log likelihood indicates that the patterns were different for the test dataset which indicates contextual anomalies. Furthermore, we managed to find more anomalies in the Summer day (6:00AM-5:59PM) where our log likelihood was -38.51211429 for the train dataset, and the test dataset had a log likelihood of -901.7588889. There was a significant difference between the two log likelihood which indicated contextual anomalies were present between this time window in the test dataset. During the weekday specifically Wednesday of the Summer between the hours (6:00 AM-5:59PM ) our HMM model detected contextual anomalies with a normalized log likelihood of -26.75830769 and after running the train model on the test data we got a normalized log likelihood of -595.8023529.

Overall in this project we managed to find point anomalies, and contextual anomalies for the summer, and parts of the winter season. We have built functions that can detect anomalies for any given seasons.

References 
[1] Eia.gov, 2018. [Online]. Available: https://www.eia.gov/todayinenergy/detail.php?id=830. [Accessed: 31- Mar- 2018].

[2] "True, Reactive, and Apparent Power | Power Factor | Electronics Textbook", Allaboutcircuits.com, 2018. [Online]. Available: https://www.allaboutcircuits.com/textbook/alternating-current/chpt-11/true-reactive-and-apparent-power/. [Accessed: 31- Mar- 2018].

[3 ]"North American Electric Reliability Corporation", En.wikipedia.org, 2018. [Online]. Available: https://en.wikipedia.org/wiki/North_American_Electric_Reliability_Corporation#/media/File:NEC-map-en.svg. [Accessed: 01- Apr- 2018].

[4]”Hacking Power Grids: A Current Problem”, wmcyberintrusion.info, 2007. [Online]. Available: http://wmcyberintrusion.info/wp-content/uploads/2017/11/HackingPowerGrids2017.pdf. [Accessed: 01-Apr-2018]

[5]Hull, J., Khurana, H., Markham, T., & Staggs, K. (2012). Staying in control: Cybersecurity and the modern electric grid. Power and Energy Magazine, IEEE,10(1), 41-48.

[6]David W. Cooke, National Research Council, Division on Engineering Physical Sciences, & Board on Energy Environmental Systems. (2013). Cybersecurity of the Grid. In The Resilience of the Electric Power Delivery System in Response to Terrorism and Natural Disasters: Summary of a Workshop (pp. 10-14). National Academies Press.

 [7] Hoang Nguyen and Klara Nahrstedt, ”Detecting Anomalies by Data Aggregation in the Power Grid”, 2006. [Ebook]. Available:https://www.semanticscholar.org/paper/Detecting-Anomalies-by-Data-Aggregation-in-the-Grid-Nguyen-Nahrstedt/c51c9a261fe0e17b82d9085ba99a996b8f43a80a [Accessed: 01 - Apr - 2018]
[8]Rui Neves-Silva，Lakhmi C. Jain，Robert J. Howlett.Intelligent Decision Technologies: Proceedings of the 7th KES International Conference on Intelligent Decision Technologies (KES-IDT 2015) (Smart Innovation, Systems and Technologies),p508
https://books.google.ca/books?id=ucvWCQAAQBAJ&pg=PA508&lpg=PA508&dq=#v=onepage&q&f=false
